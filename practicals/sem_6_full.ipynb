{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0UEY0owiWwU"
      },
      "source": [
        "# Семинар 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud3FV5xMiWwV"
      },
      "source": [
        "## Методы обработки текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFEw6IP5iWwV"
      },
      "source": [
        "Примеры задач автоматической обработки текстов:\n",
        "\n",
        "- классификация текстов\n",
        "\n",
        "    - анализ тональности\n",
        "    - фильтрация спама\n",
        "    - по теме или жанру\n",
        "\n",
        "- машинный перевод\n",
        "\n",
        "- распознавание речи\n",
        "\n",
        "- извлечение информации\n",
        "\n",
        "    - именованные сущности\n",
        "    - факты и события\n",
        "\n",
        "- кластеризация текстов\n",
        "\n",
        "- оптическое распознавание символов\n",
        "\n",
        "- проверка правописания\n",
        "\n",
        "- вопросно-ответные системы\n",
        "\n",
        "- суммаризация текстов\n",
        "\n",
        "- генерация текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVbIm2Y3iWwW"
      },
      "source": [
        "Одни из классических методов для работы с текстами:\n",
        "\n",
        "- токенизация\n",
        "\n",
        "- лемматизация / стемминг\n",
        "\n",
        "- удаление стоп-слов\n",
        "\n",
        "- векторное представление текстов (bag of words и TF-IDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiLuBDsciWwX"
      },
      "source": [
        "_Что почитать:_\n",
        "\n",
        "- Jurafsky, Martin: Speech and Language Processing (2nd or 3rd Edition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHyXvKHdiWwX"
      },
      "source": [
        "## Токенизация\n",
        "\n",
        "Токенизировать — значит, поделить текст на слова, или *токены*.\n",
        "\n",
        "Самый наивный способ токенизировать текст — разделить с помощью `split`. Но `split` упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/WeaselCMC/hse_ml_2021/raw/main/practicals/negative.csv"
      ],
      "metadata": {
        "id": "rALcq4L3ik2V",
        "outputId": "c95875fc-bacd-489e-fab0-b45ff00c16e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-13 17:44:23--  https://github.com/WeaselCMC/hse_ml_2021/raw/main/practicals/negative.csv\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/WeaselCMC/hse_ml_2021/main/practicals/negative.csv [following]\n",
            "--2022-01-13 17:44:23--  https://raw.githubusercontent.com/WeaselCMC/hse_ml_2021/main/practicals/negative.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14575645 (14M) [text/plain]\n",
            "Saving to: ‘negative.csv’\n",
            "\n",
            "negative.csv        100%[===================>]  13.90M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-01-13 17:44:24 (162 MB/s) - ‘negative.csv’ saved [14575645/14575645]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/WeaselCMC/hse_ml_2021/raw/main/practicals/positive.csv"
      ],
      "metadata": {
        "id": "RZp0OMFBinYC",
        "outputId": "9da45279-686d-45e8-ec54-0c0a2df89fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-13 17:44:42--  https://github.com/WeaselCMC/hse_ml_2021/raw/main/practicals/positive.csv\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/WeaselCMC/hse_ml_2021/main/practicals/positive.csv [following]\n",
            "--2022-01-13 17:44:42--  https://raw.githubusercontent.com/WeaselCMC/hse_ml_2021/main/practicals/positive.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16178752 (15M) [text/plain]\n",
            "Saving to: ‘positive.csv’\n",
            "\n",
            "positive.csv        100%[===================>]  15.43M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-01-13 17:44:43 (162 MB/s) - ‘positive.csv’ saved [16178752/16178752]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJD_vcYUiWwY",
        "outputId": "5fa1d446-2081-4955-8f41-ab96f5873e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tFPEKUEiWwY"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pL6mWWaiWwZ"
      },
      "outputs": [],
      "source": [
        "example = 'Но не каждый хочет что-то исправлять:('"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-c9SBLYiWwZ",
        "outputId": "f9a2a39f-dbb4-47b2-9355-04d96b031c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "example.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfTLD4DgiWwZ",
        "outputId": "72acf3a3-7f05-4db6-9987-bbc7c9cd9126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Package punkt is already up-to-date!\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# в ячейке ниже у вас может быть ошибка - надо будет загрузить пакет 'punkt'\n",
        "import nltk\n",
        "nltk.download()\n",
        "# либо nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWctyawliWwa",
        "outputId": "7128411b-3b58-4be0-d0f1-25d1fa159e6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "word_tokenize(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVupWeTniWwa"
      },
      "source": [
        "В nltk вообще есть довольно много токенизаторов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hiw556eziWwa",
        "outputId": "391a774a-083b-4f58-d933-c44ff0c8e323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC-asdOFiWwa"
      },
      "source": [
        "Они умеют выдавать индексы начала и конца каждого токена:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UZOAc4piWwb",
        "outputId": "69354461-fe8f-4b5c-c686-ea3af8d537a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "list(wh_tok.span_tokenize(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1c7ypKmiWwb"
      },
      "source": [
        "(если вам было интересно, зачем вообще включать в модуль токенизатор, который работает как `.split()` :)\n",
        "\n",
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLvUqjNoiWwb",
        "outputId": "d690b7c1-7b28-41b6-8e2d-a5faad94db96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do', \"n't\", 'stop', 'me']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6HvOVGTiWwc"
      },
      "source": [
        "Для некоторых задач это может быть полезно.\n",
        "\n",
        "А некоторые — вообще не для текста на естественном языке:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeh3jAd_iWwc",
        "outputId": "0c41d036-f648-4fc3-e0e8-b3764eb306ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(a (b c))', 'd', 'e', '(f)']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0Tmtzs8iWwd",
        "outputId": "5cf41505-2834-494f-af71-09ac09c00622",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':(']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tw = TweetTokenizer()\n",
        "tw.tokenize(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWRug03ciWwd"
      },
      "source": [
        "_Что почитать:_\n",
        "\n",
        "- http://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/\n",
        "- https://blog.floydhub.com/tokenization-nlp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXg2C8u7iWwd"
      },
      "source": [
        "## Стоп-слова и пунктуация\n",
        "\n",
        "*Стоп-слова* — это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конкретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX4bvjIriWwd",
        "outputId": "1ac85d52-320a-425e-958c-f0939e77256e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUPNVQwiiWwe",
        "outputId": "9aed1f15-9579-422b-862d-200512382d60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('russian'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ye2gQB9ViWwe",
        "outputId": "777437d4-4b0e-4313-cff0-136ea73a91e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMNdHFVsiWwe"
      },
      "outputs": [],
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JMXdTcDiWwe"
      },
      "source": [
        "## Лемматизация\n",
        "\n",
        "Лемматизация — это сведение разных форм одного слова к начальной форме — *лемме*. Например, токены «пью», «пил», «пьет» перейдут в «пить». Почему это хорошо?\n",
        "* Во-первых, мы хотим рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
        "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лемматизации мы исключаем только её.\n",
        "\n",
        "Неплохие лемматизаторы для русского языка — `mystem` и `pymorphy`.\n",
        "\n",
        "### [Mystem](https://tech.yandex.ru/mystem/)\n",
        "\n",
        "Как с ним работать:\n",
        "* можно скачать `mystem` и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
        "* [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) — обертка для питона, работает медленнее, но это удобно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmOEDy5jiWwf",
        "outputId": "c269eb08-f970-4524-b55c-69edfb04e004",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymystem3==0.1.10\n",
            "  Downloading pymystem3-0.1.10-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pymystem3==0.1.10) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pymystem3==0.1.10) (2021.10.8)\n",
            "Installing collected packages: pymystem3\n",
            "  Attempting uninstall: pymystem3\n",
            "    Found existing installation: pymystem3 0.2.0\n",
            "    Uninstalling pymystem3-0.2.0:\n",
            "      Successfully uninstalled pymystem3-0.2.0\n",
            "Successfully installed pymystem3-0.1.10\n"
          ]
        }
      ],
      "source": [
        "!pip install pymystem3==0.1.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMwqMwNFiWwf"
      },
      "outputs": [],
      "source": [
        "from pymystem3 import Mystem\n",
        "mystem_analyzer = Mystem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvNLnLATiWwf"
      },
      "source": [
        "Мы инициализировали `Mystem` c параметрами по умолчанию. А вообще параметры есть такие:\n",
        "* mystem_bin — путь к `mystem`, если их несколько\n",
        "* grammar_info — нужна ли грамматическая информация или только леммы (по умолчанию нужна)\n",
        "* disambiguation — нужно ли снятие омонимии — дизамбигуация (по умолчанию нужна)\n",
        "* entire_input — нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по умолчанию оставляется все)\n",
        "\n",
        "Методы `Mystem` принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
        "\n",
        "Можно просто лемматизировать текст:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJHduT-1iWwf",
        "outputId": "1d68238f-c1de-4f69-8a09-bd54c9aa8b45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2365ae7a9b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmystem_analyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mneed_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pymystem3/mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_mystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_NL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ],
      "source": [
        "print(mystem_analyzer.lemmatize(example[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7QXrt_AiWwg"
      },
      "source": [
        "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahWUqmUNiWwg",
        "outputId": "e6295c60-8da1-4f52-f1a9-7584fc798709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: pymorphy2-dicts in /usr/local/lib/python3.7/dist-packages (2.4.393442.3710985)\n",
            "Requirement already satisfied: DAWG-Python in /usr/local/lib/python3.7/dist-packages (0.7.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2\n",
        "!pip install pymorphy2-dicts\n",
        "!pip install DAWG-Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmHoCaaSiWwg"
      },
      "outputs": [],
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m2rZV2jiWwh"
      },
      "source": [
        "`pymorphy2` работает с отдельными словами. Если дать ему на вход предложение — он его просто не лемматизирует, т.к. не понимает."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAkkFoI_iWwi"
      },
      "outputs": [],
      "source": [
        "tokenized_example = tw.tokenize(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsDnfre0iWwi",
        "outputId": "46920723-c28a-4d84-d5fd-05de208d89e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'хочет'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "tokenized_example[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txE9dsvQiWwj",
        "outputId": "2943a22c-54a7-4cab-939a-7a4ae9d5bea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='хочет', tag=OpencorporaTag('VERB,impf,tran sing,3per,pres,indc'), normal_form='хотеть', score=1.0, methods_stack=((DictionaryAnalyzer(), 'хочет', 3136, 5),))]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "ana = pymorphy2_analyzer.parse(tokenized_example[3])\n",
        "ana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn2vCJr1iWwj",
        "outputId": "1dd7bf88-5457-4857-a91a-81428afbdc99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'хотеть'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "ana[0].normal_form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2W5mCB7iWwj"
      },
      "source": [
        "### mystem vs. pymorphy\n",
        "\n",
        "1) *Скорость.* `mystem` работает невероятно медленно под Windows на больших текстах\n",
        "\n",
        "2) *Снятие омонимии*. `Mystem` умеет снимать омонимию по контексту (хотя не всегда преуспевает), `pymorphy2` берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5nlvURDiWwj"
      },
      "outputs": [],
      "source": [
        "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
        "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
        "mystem_analyzer = Mystem() # инициализируем объект с параметрами по умолчанию\n",
        "\n",
        "print(mystem_analyzer.analyze(homonym1)[-5])\n",
        "print(mystem_analyzer.analyze(homonym2)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZiUQjlZiWwk"
      },
      "outputs": [],
      "source": [
        "mystem_analyzer.lemmatize(homonym2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC6Xehd1iWwk"
      },
      "source": [
        "## Стемминг"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BiDIrbxiWwl"
      },
      "source": [
        "В отличие от лемматизации, при применении стемминга у всех слов отбрасываются аффиксы (окончания и суффиксы), что необязательно приводит слова к формам, существующим в рассматриваемом языке."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URDp9AeliWwl",
        "outputId": "a0d04a48-6d4a-4747-fe42-6e8ce779fe3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\n",
            "\"Whenever you feel like criticizing any one,\" he told me, \"just remember that all the people in this world haven't had the advantages that you've had.\"\n"
          ]
        }
      ],
      "source": [
        "text = \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\\n\\\"Whenever you feel like criticizing any one,\\\" he told me, \\\"just remember that all the people in this world haven't had the advantages that you've had.\\\"\"\n",
        "print(text)\n",
        "text_tokenized = [w for w in word_tokenize(text) if w.isalpha()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBS5wxLwiWwm",
        "outputId": "9e5b43ca-b62e-459f-da2e-d666bb1cdf6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in my younger and more vulner year my father gave me some advic that i been turn over in my mind ever sinc whenev you feel like critic ani one he told me just rememb that all the peopl in this world have had the advantag that you had\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "text_stemmed = [stemmer.stem(w) for w in text_tokenized]\n",
        "print(' '.join(text_stemmed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMtIc3nHiWwm",
        "outputId": "44c25e87-bba4-48f4-c898-0f96b3d26229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#nltk.download()\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVgFM4dOiWwm",
        "outputId": "664c5aab-0f83-42a4-8f34-806fb3d80ab8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In my younger and more vulnerable year my father gave me some advice that I been turning over in my mind ever since Whenever you feel like criticizing any one he told me just remember that all the people in this world have had the advantage that you had\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "text_lemmatized = [lemmatizer.lemmatize(w) for w in text_tokenized]\n",
        "print(' '.join(text_lemmatized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpYXcW2liWwm"
      },
      "source": [
        "_Что почитать:_\n",
        "\n",
        "- https://en.wikipedia.org/wiki/Stemming\n",
        "- https://en.wikipedia.org/wiki/Lemmatisation\n",
        "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NCxYpDJiWwm"
      },
      "source": [
        "## Bag-of-words и TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1LzEIDXiWwm"
      },
      "source": [
        "Но как же все-таки работать с текстами, используя стандартные методы машинного обучения? Нужна выборка!\n",
        "\n",
        "Модель bag-of-words: текст можно представить как набор независимых слов. Тогда каждому слову можно сопоставить вес, таким образом, сопоставляя тексту набор весов. В качестве весов можно брать частоту встречаемости слов в тексте."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3GBvnIFiWwm"
      },
      "outputs": [],
      "source": [
        "texts = ['I like my cat.', 'My cat is the most perfect cat.', 'is this cat or is this bread?']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrtMPg2-iWwm",
        "outputId": "28992f88-bd51-4a02-beeb-4e2fa31184fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I like my cat',\n",
              " 'My cat is the most perfect cat',\n",
              " 'is this cat or is this bread']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "texts_tokenized = [' '.join([w for w in word_tokenize(t) if w.isalpha()]) for t in texts]\n",
        "texts_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnrZwd53iWwm"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cnt_vec = CountVectorizer()\n",
        "X = cnt_vec.fit_transform(texts_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6stS6I1iWwn",
        "outputId": "1ee523bb-741a-4323-ae57-a5fca14d79c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bread', 'cat', 'is', 'like', 'most', 'my', 'or', 'perfect', 'the', 'this']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "cnt_vec.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3djjthHiWwn",
        "outputId": "8cf57963-9163-4e96-e2cc-2a2f389f7a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 14 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgsDCx23iWwn",
        "outputId": "7ad93514-5939-4bd0-c9d6-2853d3c7fbf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
              "       [0, 2, 1, 0, 1, 1, 0, 1, 1, 0],\n",
              "       [1, 1, 2, 0, 0, 0, 1, 0, 0, 2]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "X.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f_fIpl_iWwn"
      },
      "source": [
        "Тексты:\n",
        "\n",
        "- I like my cat.\n",
        "- My cat is the most perfect cat.\n",
        "- is this cat or is this bread?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf9tZuUUiWwn",
        "outputId": "e074d247-6468-459b-b754-b638e922e6b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-af405e5d-0750-420c-b007-8b874e1b3e50\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bread</th>\n",
              "      <th>cat</th>\n",
              "      <th>is</th>\n",
              "      <th>like</th>\n",
              "      <th>most</th>\n",
              "      <th>my</th>\n",
              "      <th>or</th>\n",
              "      <th>perfect</th>\n",
              "      <th>the</th>\n",
              "      <th>this</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af405e5d-0750-420c-b007-8b874e1b3e50')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-af405e5d-0750-420c-b007-8b874e1b3e50 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-af405e5d-0750-420c-b007-8b874e1b3e50');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   bread  cat  is  like  most  my  or  perfect  the  this\n",
              "0      0    1   0     1     0   1   0        0    0     0\n",
              "1      0    2   1     0     1   1   0        1    1     0\n",
              "2      1    1   2     0     0   0   1        0    0     2"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "pd.DataFrame(X.toarray(), columns=cnt_vec.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPQz5PY1iWwn"
      },
      "source": [
        "Заметим, что если слово часто встречается в одном тексте, но почти не встречается в других, то оно получает для данного текста большой вес, ровно так же, как и слова, которые часто встречаются в каждом тексте. Для того, чтобы разделять эти такие слова, можно использовать статистическую меру TF-IDF, характеризующую важность слова для конкретного текста. Для каждого слова из текста $d$ рассчитаем относительную частоту встречаемости в нем (Term Frequency):\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{C(t | d)}{\\sum\\limits_{k \\in d}C(k | d)},\n",
        "$$\n",
        "где $C(t | d)$ - число вхождений слова $t$ в текст $d$.\n",
        "\n",
        "Также для каждого слова из текста $d$ рассчитаем обратную частоту встречаемости в корпусе текстов $D$ (Inverse Document Frequency):\n",
        "$$\n",
        "\\text{IDF}(t, D) = \\log\\left(\\frac{|D|}{|\\{d_i \\in D \\mid t \\in d_i\\}|}\\right)\n",
        "$$\n",
        "Логарифмирование здесь проводится с целью уменьшить масштаб весов, ибо зачастую в корпусах присутствует очень много текстов.\n",
        "\n",
        "В итоге каждому слову $t$ из текста $d$ теперь можно присвоить вес\n",
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "$$\n",
        "Интерпретировать формулу выше несложно: действительно, чем чаще данное слово встречается в данном тексте и чем реже в остальных, тем важнее оно для этого текста.\n",
        "\n",
        "Отметим, что в качестве TF и IDF можно использовать другие определения (https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4PhrKdbiWwn"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "X = tfidf_vec.fit_transform(texts_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGrrhueRiWwo",
        "outputId": "1d946343-89dd-427b-d720-4059c05178d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bread', 'cat', 'is', 'like', 'most', 'my', 'or', 'perfect', 'the', 'this']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "tfidf_vec.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKC1tY32iWwo",
        "outputId": "40694958-5e8f-4426-fc4c-a32bf6db8666",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x10 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 14 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9TpH5gJiWwo",
        "outputId": "ba0f3df3-cda9-4a80-ed93-3020910a7f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.42544054, 0.        , 0.72033345, 0.        ,\n",
              "        0.54783215, 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.50130994, 0.32276391, 0.        , 0.42439575,\n",
              "        0.32276391, 0.        , 0.42439575, 0.42439575, 0.        ],\n",
              "       [0.33976626, 0.20067143, 0.516802  , 0.        , 0.        ,\n",
              "        0.        , 0.33976626, 0.        , 0.        , 0.67953252]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "X.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fEItFzviWwo"
      },
      "source": [
        "Тексты:\n",
        "\n",
        "- I like my cat.\n",
        "- My cat is the most perfect cat.\n",
        "- is this cat or is this bread?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyIyCU8oiWwo",
        "outputId": "2fa89b0f-687a-4e06-c653-59a42f77f40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-03fa79a0-3c82-40e8-bfe8-08927a421756\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bread</th>\n",
              "      <th>cat</th>\n",
              "      <th>is</th>\n",
              "      <th>like</th>\n",
              "      <th>most</th>\n",
              "      <th>my</th>\n",
              "      <th>or</th>\n",
              "      <th>perfect</th>\n",
              "      <th>the</th>\n",
              "      <th>this</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.425441</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.720333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.547832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.501310</td>\n",
              "      <td>0.322764</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.424396</td>\n",
              "      <td>0.322764</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.424396</td>\n",
              "      <td>0.424396</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.339766</td>\n",
              "      <td>0.200671</td>\n",
              "      <td>0.516802</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.339766</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.679533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03fa79a0-3c82-40e8-bfe8-08927a421756')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-03fa79a0-3c82-40e8-bfe8-08927a421756 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-03fa79a0-3c82-40e8-bfe8-08927a421756');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      bread       cat        is  ...   perfect       the      this\n",
              "0  0.000000  0.425441  0.000000  ...  0.000000  0.000000  0.000000\n",
              "1  0.000000  0.501310  0.322764  ...  0.424396  0.424396  0.000000\n",
              "2  0.339766  0.200671  0.516802  ...  0.000000  0.000000  0.679533\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "pd.DataFrame(X.toarray(), columns=tfidf_vec.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7mmET-ZiWwo"
      },
      "source": [
        "Что изменилось по сравнению с методом `CountVectorizer`? Интерпретируйте результат."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXT0JlbFiWwo"
      },
      "source": [
        "_Что почитать:_\n",
        "\n",
        "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
        "- https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z48FpsPOiWwo"
      },
      "source": [
        "## n-grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBywoGl7iWwo"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ph_4LrRiWwo"
      },
      "source": [
        "Что такое n-граммы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmNlquGkiWwo"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcnzNu0PiWwp",
        "outputId": "0f2067da-c6ae-46d6-9eec-2b52c75dce6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "sent = 'Если б мне платили каждый раз'.split()\n",
        "list(ngrams(sent, 1)) # униграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4br3-8B8iWwp",
        "outputId": "66e2dc8a-4e2f-4657-9e70-79f623f2fe84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б'),\n",
              " ('б', 'мне'),\n",
              " ('мне', 'платили'),\n",
              " ('платили', 'каждый'),\n",
              " ('каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "list(ngrams(sent, 2)) # биграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0Savj2LiWwp",
        "outputId": "250afc36-9f14-44b5-ba02-eb58afc5d75f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне'),\n",
              " ('б', 'мне', 'платили'),\n",
              " ('мне', 'платили', 'каждый'),\n",
              " ('платили', 'каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "list(ngrams(sent, 3)) # триграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "D1heTymoiWwp",
        "outputId": "3fa62cc5-2734-4bc5-c9f9-d3ea0c3f5742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
              " ('б', 'мне', 'платили', 'каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "list(ngrams(sent, 5)) # ... пентаграммы?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2dJMS6niWwp"
      },
      "source": [
        "## Задача: классификация твитов по тональности\n",
        "\n",
        "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
        "\n",
        "Классификацию по тональности используют в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
        "\n",
        "[Источник датасета](http://study.mokoron.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZHtiOAiiWwp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GqqKcHQiWwp"
      },
      "outputs": [],
      "source": [
        "# считываем данные и заполняем общий датасет\n",
        "positive = pd.read_csv('positive.csv')\n",
        "positive['label'] = ['positive'] * len(positive)\n",
        "negative = pd.read_csv('negative.csv')\n",
        "negative['label'] = ['negative'] * len(negative)\n",
        "df = positive.append(negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIFuGWAMiWwp",
        "outputId": "f282cc7e-93fd-4d26-c1ae-f5ef1f8b3de5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2496c2f0-1ba8-40b4-aa32-5992deffc6dc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>55833</th>\n",
              "      <td>Супер освещенна дорога домой:D таки дела. http...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91795</th>\n",
              "      <td>теперь и спальня украшена ) @ MargaritaAlegria...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14214</th>\n",
              "      <td>@not_believer Примерно с таким лицом я тогда п...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85976</th>\n",
              "      <td>Жутко замерзли, много фруктов и- я шлюха-прям,...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63979</th>\n",
              "      <td>RT @Diana17_11: @Renatkaaaaaaaa аа блин точно....</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55673</th>\n",
              "      <td>RT @a_poloskov: \"@olgavaryushina: Счастливый с...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76911</th>\n",
              "      <td>@6ilguun цагаан хар алаглуулаад будчихгүй юу т...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104203</th>\n",
              "      <td>Пришел, станцевал минимум, ушел работать) #сал...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87473</th>\n",
              "      <td>Проснулась, а на улице дождь. Офигенный январь(((</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>Пятница вечер. И снова отмечаем) http://t.co/c...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2496c2f0-1ba8-40b4-aa32-5992deffc6dc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2496c2f0-1ba8-40b4-aa32-5992deffc6dc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2496c2f0-1ba8-40b4-aa32-5992deffc6dc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                     text     label\n",
              "55833   Супер освещенна дорога домой:D таки дела. http...  positive\n",
              "91795   теперь и спальня украшена ) @ MargaritaAlegria...  positive\n",
              "14214   @not_believer Примерно с таким лицом я тогда п...  positive\n",
              "85976   Жутко замерзли, много фруктов и- я шлюха-прям,...  positive\n",
              "63979   RT @Diana17_11: @Renatkaaaaaaaa аа блин точно....  negative\n",
              "55673   RT @a_poloskov: \"@olgavaryushina: Счастливый с...  positive\n",
              "76911   @6ilguun цагаан хар алаглуулаад будчихгүй юу т...  positive\n",
              "104203  Пришел, станцевал минимум, ушел работать) #сал...  positive\n",
              "87473   Проснулась, а на улице дождь. Офигенный январь(((  negative\n",
              "795     Пятница вечер. И снова отмечаем) http://t.co/c...  positive"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-N4k0gWiWwp",
        "outputId": "5679b6d9-c8aa-4b96-c171-b1e1a929737f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(226834, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZTyE_81iWwp"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df['text'], df['label'], random_state=13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQRZP_yiiWwq"
      },
      "source": [
        "Самый простой способ извлечь фичи из текстовых данных — векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
        "\n",
        "Объект `CountVectorizer` делает простую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` — количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz_lLRxWiWwq"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train) # bow — bag of words (мешок слов)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al1R646liWwq",
        "outputId": "9f50157e-55d7-43ac-c72c-27e908e40582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<170125x243421 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1847716 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rt6YmKEiWwq"
      },
      "source": [
        "`'ngram_range'` отвечает за то, какие n-граммы мы используем в качестве признаков:\n",
        "\n",
        "- `'ngram_range'=(1, 1)` — униграммы\n",
        "- `'ngram_range'=(3, 3)` — триграммы\n",
        "- `'ngram_range'=(1, 3)` — униграммы, биграммы и триграммы\n",
        "\n",
        "В `vec.vocabulary_` лежит словарь — отображение слов в индексы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxqNVCRuiWwq",
        "outputId": "4ad0f238-8fb3-4399-ffec-27f534221ebc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('690064', 4548),\n",
              " ('давай', 122764),\n",
              " ('ходить', 233469),\n",
              " ('со', 212154),\n",
              " ('мной', 159037),\n",
              " ('будешь', 108431),\n",
              " ('отпрашиваться', 176489),\n",
              " ('на', 161516),\n",
              " ('два', 123438),\n",
              " ('часа', 235947)]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "list(vec.vocabulary_.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cjHMvNKiWwq",
        "outputId": "30c9abfa-2fe8-4482-e3bc-c15cdfca84d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=13)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=13)\n",
        "clf.fit(bow, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "N7BJY79qiWwq",
        "outputId": "8cf6d568-5698-49f2-b1df-690145e440b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.76      0.76     28181\n",
            "    positive       0.76      0.77      0.76     28528\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFtON4HjiWwq"
      },
      "source": [
        "Попробуем сделать то же самое для униграмм и биграмм:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "??LogisticRegression"
      ],
      "metadata": {
        "id": "bz5YWbckukgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "FIDs-qoziWwq",
        "outputId": "2c0d036b-75d6-40cc-fa77-721f302d9be1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.78      0.78     27984\n",
            "    positive       0.78      0.78      0.78     28725\n",
            "\n",
            "    accuracy                           0.78     56709\n",
            "   macro avg       0.78      0.78      0.78     56709\n",
            "weighted avg       0.78      0.78      0.78     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 2))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=13, penalty='l2', solver='lbfgs',  max_iter=200, n_jobs=-1, warm_start=True)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmepHKTdiWwq"
      },
      "source": [
        "А теперь для TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVYxhXGKiWwq",
        "outputId": "2195aee5-8fdf-4c03-b88a-684807c6985c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.73      0.77      0.75     26695\n",
            "    positive       0.78      0.75      0.77     30014\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=13, penalty='l2',\n",
        "                         solver='lbfgs',  max_iter=200, n_jobs=-1, warm_start=True)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDjLZ_o8iWwq"
      },
      "source": [
        "## О важности анализа данных\n",
        "\n",
        "Но иногда пунктуация бывает и не шумом. Главное — отталкиваться от задачи. Что будет, если вообще не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN9xxUPOiWwr",
        "outputId": "c1c7fa0a-a0b8-4a5f-cbfd-7066df7eb8bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     27924\n",
            "    positive       1.00      1.00      1.00     28785\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=word_tokenize)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=13)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMigIRB9iWwr"
      },
      "source": [
        "Стоило оставить пунктуацию — и внезапно все метрики устремились к 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите признаки с самыми большими коэффициентами:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argsort([1,2,3,4])"
      ],
      "metadata": {
        "id": "gyr6vetbx3S2",
        "outputId": "fc420b88-b343-4cd2-cda9-e5db24179344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9USMnfaiWwr"
      },
      "outputs": [],
      "source": [
        "coef = np.abs(clf.coef_.flatten())\n",
        "args_coef = np.argsort(-coef)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF1-pq2SiWwr"
      },
      "source": [
        "Посмотрим на твиты с данным токеном:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[(vec.get_feature_names()[args_coef[::-1][k]], coef[args_coef[::-1][k]]) for k in range(5)]"
      ],
      "metadata": {
        "id": "Fc-aZqaPysZQ",
        "outputId": "6e2c0b03-fe79-4920-bcd9-070122c24d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('greeenmary', 6.696052813921826e-09),\n",
              " ('брайант', 1.19174297612788e-08),\n",
              " ('тривиальный', 2.1183359472253876e-08),\n",
              " ('puuuuudge', 2.9488950698693746e-08),\n",
              " ('сметанку', 3.9326628107180015e-08)]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_knU4WkliWwr",
        "outputId": "554009fb-6a59-4622-875b-6ad45eda7d20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@nastasja_krush дааа*__* хоть что-то радует хд но я вщ ссу:((\n",
            "А помните как раньше было \"Обновлено ... дней назад\". Сейчас такого уже нет( http://t.co/jQy61YnQXR\n",
            "Пздец, а не ГИА! Дебильные задания! Тупая математика!(((\n",
            "@anitatsoi уже 2-й за последние несколько месяцев (((( страшно. Мой город.\n",
            "Еду к @____Yume____  но не могу до нее дозвониться(\n",
            "бедные мои вочы :( http://t.co/RYOcKWdYTI\n",
            "@hrustalev_k дорогой, дело твое, просто мне будет очень грустно, если превратишься в запойного:(\n",
            "Брат едет домой, а я в городе, печалька((значит нужно сделать сюрприз***\n",
            "Если рано проснутся, то можно сделать все дела за полдня! А потом маяться бездельем:( http://t.co/JjeWoXcbFt\n",
            "@asyaanderson бумажные мне теперь заказаны, если не в Букхантере ( почтового адреса нет\n"
          ]
        }
      ],
      "source": [
        "cool_token = vec.get_feature_names()[args_coef[0]]\n",
        "tweets_with_cool_token = [tweet for tweet in x_train if cool_token in tweet]\n",
        "np.random.seed(42)\n",
        "for tweet in np.random.choice(tweets_with_cool_token, size=10, replace=False):\n",
        "    print(tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16mUIe4JiWwr"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cool_token"
      ],
      "metadata": {
        "id": "tzOqNbMvzXTs",
        "outputId": "db9dbe10-ed12-40a9-83e1-75bcc04d3c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'('"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX9rAOgKiWwr",
        "outputId": "aa66daa1-21a5-4d68-ec45-ee1cfffe1f34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.95      1.00      0.97     26439\n",
            "    positive       1.00      0.95      0.97     30270\n",
            "\n",
            "    accuracy                           0.97     56709\n",
            "   macro avg       0.97      0.97      0.97     56709\n",
            "weighted avg       0.97      0.97      0.97     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cool_token = vec.get_feature_names()[args_coef[0]]\n",
        "pred = ['negative' if cool_token in tweet else 'positive' for tweet in x_test]\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWmvB0SqiWwr"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "Теперь в качестве фичей используем, например, униграммы символов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-ItvbX6iWws"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=13)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_AZC5z_iWws"
      },
      "source": [
        "В общем-то, теперь уже понятно, почему на этих данных здесь получается очень хорошее качество. Так или иначе, на символах классифицировать тоже можно: для некоторых задач (например, для определения языка) признаки-символьные n-граммы могут внести серьезный вклад в качество модели.\n",
        "\n",
        "Ещё одна замечательная особенность признаков-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9nR8dCwiWws"
      },
      "source": [
        "_Что почитать:_\n",
        "\n",
        "- https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
        "- https://books.google.com/ngrams"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of sem_6_empty.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}